<!DOCTYPE html>
<html lang="it">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gemini Voice</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
        }

        .container {
            text-align: center;
            padding: 2rem;
        }

        h1 {
            font-size: 3rem;
            margin-bottom: 1rem;
        }

        #status {
            font-size: 1.2rem;
            margin: 2rem 0;
            opacity: 0.9;
        }

        button {
            padding: 1.5rem 3rem;
            font-size: 1.3rem;
            font-weight: 600;
            color: white;
            background: rgba(255, 255, 255, 0.2);
            border: 2px solid white;
            border-radius: 50px;
            cursor: pointer;
            transition: all 0.3s;
        }

        button:hover {
            background: rgba(255, 255, 255, 0.3);
            transform: scale(1.05);
        }

        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }

        .recording {
            background: rgba(234, 67, 53, 0.3);
            animation: pulse 2s infinite;
        }

        @keyframes pulse {

            0%,
            100% {
                box-shadow: 0 0 20px rgba(234, 67, 53, 0.5);
            }

            50% {
                box-shadow: 0 0 40px rgba(234, 67, 53, 0.8);
            }
        }

        #response {
            margin-top: 2rem;
            padding: 1.5rem;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 15px;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>üéôÔ∏è Gemini Voice</h1>
        <div id="status">Caricamento...</div>
        <button id="recordBtn" disabled>Parla</button>
        <div id="response"></div>
    </div>

    <script type="module">
        import { GoogleGenAI, Modality } from 'https://esm.run/@google/genai';

        const statusEl = document.getElementById('status');
        const recordBtn = document.getElementById('recordBtn');
        const responseEl = document.getElementById('response');

        let session = null;
        let isRecording = false;
        let audioContext = null;
        let processor = null;
        let stream = null;
        let playbackContext = null;
        let nextPlayTime = 0;

        // Inizializza Gemini
        async function init() {
            try {
                const apiKey = prompt('Inserisci la tua API Key di Gemini:');
                if (!apiKey) {
                    statusEl.textContent = 'API Key richiesta';
                    return;
                }

                const ai = new GoogleGenAI({ apiKey });

                // Inizializza AudioContext per playback
                playbackContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 24000 });

                session = await ai.live.connect({
                    model: 'gemini-2.5-flash-native-audio-preview-09-2025',
                    config: {
                        responseModalities: [Modality.AUDIO]
                    },
                    callbacks: {
                        onopen: () => console.log('Connesso'),
                        onmessage: handleMessage,
                        onerror: (e) => statusEl.textContent = 'Errore: ' + e.message,
                        onclose: () => statusEl.textContent = 'Disconnesso'
                    }
                });

                statusEl.textContent = 'Pronto! Clicca per parlare';
                recordBtn.disabled = false;

            } catch (error) {
                statusEl.textContent = 'Errore: ' + error.message;
            }
        }

        function handleMessage(message) {
            console.log('Messaggio:', message);

            if (message.setupComplete) {
                console.log('Setup completato');
                return;
            }

            if (message.data) {
                playAudioChunk(message.data);
            }

            if (message.text) {
                responseEl.textContent = message.text;
            }

            if (message.serverContent?.turnComplete) {
                statusEl.textContent = 'Pronto! Clicca per parlare';
                // Reset play time for next response
                nextPlayTime = 0;
            }
        }

        async function playAudioChunk(base64Data) {
            try {
                const binaryString = atob(base64Data);
                const bytes = new Uint8Array(binaryString.length);
                for (let i = 0; i < binaryString.length; i++) {
                    bytes[i] = binaryString.charCodeAt(i);
                }

                const pcm16 = new Int16Array(bytes.buffer);
                const audioBuffer = playbackContext.createBuffer(1, pcm16.length, 24000);
                const channelData = audioBuffer.getChannelData(0);

                for (let i = 0; i < pcm16.length; i++) {
                    channelData[i] = pcm16[i] / 32768.0;
                }

                const source = playbackContext.createBufferSource();
                source.buffer = audioBuffer;
                source.connect(playbackContext.destination);

                // Schedule playback in queue
                const currentTime = playbackContext.currentTime;
                const startTime = Math.max(currentTime, nextPlayTime);
                source.start(startTime);

                // Update next play time
                nextPlayTime = startTime + audioBuffer.duration;

                statusEl.textContent = 'Gemini sta parlando...';

            } catch (error) {
                console.error('Errore audio:', error);
            }
        }

        async function startRecording() {
            try {
                stream = await navigator.mediaDevices.getUserMedia({
                    audio: { sampleRate: 16000, channelCount: 1 }
                });

                audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
                const source = audioContext.createMediaStreamSource(stream);
                processor = audioContext.createScriptProcessor(4096, 1, 1);

                source.connect(processor);
                processor.connect(audioContext.destination);

                processor.onaudioprocess = (e) => {
                    if (!isRecording) return;

                    const inputData = e.inputBuffer.getChannelData(0);
                    const pcmData = new Int16Array(inputData.length);
                    for (let i = 0; i < inputData.length; i++) {
                        pcmData[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
                    }

                    const bytes = new Uint8Array(pcmData.buffer);
                    let binary = '';
                    for (let i = 0; i < bytes.byteLength; i++) {
                        binary += String.fromCharCode(bytes[i]);
                    }
                    const base64 = btoa(binary);

                    session.sendRealtimeInput({
                        audio: { data: base64, mimeType: 'audio/pcm;rate=16000' }
                    });
                };

                isRecording = true;
                recordBtn.textContent = 'Stop';
                recordBtn.classList.add('recording');
                statusEl.textContent = 'Ti ascolto...';
                responseEl.textContent = '';

            } catch (error) {
                statusEl.textContent = 'Errore microfono: ' + error.message;
            }
        }

        function stopRecording() {
            isRecording = false;

            if (processor) processor.disconnect();
            if (stream) stream.getTracks().forEach(track => track.stop());
            if (audioContext) audioContext.close();

            session.sendRealtimeInput({ audioStreamEnd: true });

            recordBtn.textContent = 'Parla';
            recordBtn.classList.remove('recording');
            statusEl.textContent = 'Elaborazione...';
        }

        recordBtn.addEventListener('click', () => {
            if (isRecording) {
                stopRecording();
            } else {
                startRecording();
            }
        });

        init();
    </script>
</body>

</html>